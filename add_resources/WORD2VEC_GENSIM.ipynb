{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#WORD2VEC\n",
    "\n",
    "## Working with Word2Vec with Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been working with a number of techniques and tools that help us navigate the world of NLP.       \n",
    "**For example, we have our Vectorizer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>above</th>\n",
       "      <th>above all</th>\n",
       "      <th>all</th>\n",
       "      <th>all to</th>\n",
       "      <th>be</th>\n",
       "      <th>be true</th>\n",
       "      <th>come</th>\n",
       "      <th>come to</th>\n",
       "      <th>denmark</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>the state</th>\n",
       "      <th>thine</th>\n",
       "      <th>thine own</th>\n",
       "      <th>this</th>\n",
       "      <th>this above</th>\n",
       "      <th>to</th>\n",
       "      <th>to thine</th>\n",
       "      <th>to this</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   above  above all  all  all to  be  be true  come  come to  denmark  in  \\\n",
       "0      0          0    0       0   0        0     1        1        0   0   \n",
       "1      1          1    1       1   1        1     0        0        0   0   \n",
       "2      0          0    0       0   0        0     0        0        1   1   \n",
       "\n",
       "   ...   the  the state  thine  thine own  this  this above  to  to thine  \\\n",
       "0  ...     0          0      0          0     1           0   1         0   \n",
       "1  ...     0          0      1          1     1           1   1         1   \n",
       "2  ...     1          1      0          0     0           0   0         0   \n",
       "\n",
       "   to this  true  \n",
       "0        1     0  \n",
       "1        0     1  \n",
       "2        0     0  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "vectorizer.fit(text)\n",
    "x = vectorizer.transform(text)\n",
    "x_back = x.toarray()\n",
    "\n",
    "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Two Adverse Traits abou the Bag of Words model:**      \n",
    "1)  Word Context & semantic meaning does not play a role   \n",
    "2)  Our data size increases with vocabulary size. \n",
    "\n",
    "** And then came Word2Vec** \n",
    "\n",
    "We will see that with Word2Vec context does play a role and it can decipher the relationship between words including: \n",
    "\n",
    "linguistic relationships:  (e.g., “vector(‘king’) – vector(‘man’) + vector(‘woman’) =~ (‘queen’))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First things first :\n",
    "\n",
    "\n",
    "**1) Install Gensim: **\n",
    "\n",
    "pip install gensim\n",
    "\n",
    "**  2)  Make sure cython is installed ? **\n",
    "\n",
    "cython -V\n",
    "\n",
    "(if no cython):\n",
    "\n",
    "pip install cython\n",
    "\n",
    "\n",
    "** 3) test (run the following) **\n",
    "\n",
    "from gensim import Word2Vec\n",
    "text = [['testin','testing','testing']]\n",
    "model = Word2Vec(text,workers=4)\n",
    "\n",
    "**4) If you see the following error : \"UserWarning: C extension not loaded for Word2Vec\"**\n",
    "\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1.  pip uninstall gensim\n",
    "2.  pip uninstall scipy \n",
    "\n",
    "3. pip install --no-cache-dir scipy==0.15.1\n",
    "4. pip install --no-cache-dir gensim==0.12.1\n",
    "\n",
    "\n",
    "**Refer to the following:** https://groups.google.com/forum/#!topic/gensim/isBqIhrw9mk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  A 'Gensim' example: \n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word2Vec (In a Word..)\n",
    "\n",
    "###  Idea 1: Preprocessing\n",
    "\n",
    "1) Tokenization   \n",
    "2) Remove stop words    \n",
    "3) Convert to lowercase     \n",
    "4) Others: stemming.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# The type of input that Word2Vec is looking for.. \n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "print texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Idea 2: Word Representation\n",
    "\n",
    "Learn a continuous representation of words.\n",
    "Each word (w) is associated with it's own word vector C(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.60290846e-03   2.64952192e-03  -4.01750021e-03   1.51745591e-03\n",
      "  -1.94312329e-03   4.38236195e-04  -1.41889264e-04   1.34513830e-03\n",
      "  -4.01126919e-03   1.00765983e-03  -2.78462586e-03   2.17301771e-03\n",
      "  -2.22336175e-03  -2.83751870e-03  -1.49124057e-03   4.26495587e-03\n",
      "   1.51252816e-03  -3.69674829e-03  -3.60761560e-03   1.50641694e-03\n",
      "   2.96684972e-04   1.45111303e-03  -3.22056771e-03  -4.79736738e-03\n",
      "   6.92113303e-04   4.17999458e-03  -4.27227514e-03   6.63728337e-04\n",
      "  -8.40774272e-04  -5.01733180e-03  -3.63163697e-03   1.31287775e-03\n",
      "  -4.52222209e-03  -3.73024936e-03  -2.75684590e-03   4.95271757e-03\n",
      "  -2.78775464e-03   3.16175050e-03   3.49858007e-03  -2.28901766e-03\n",
      "  -1.11150870e-03  -2.29695300e-03   3.72917828e-04  -8.70059594e-04\n",
      "  -4.48034843e-04  -3.86656355e-03  -1.96747994e-03  -2.37318990e-03\n",
      "  -3.50443041e-03  -1.47374463e-03   1.15248945e-03   4.21493221e-03\n",
      "   3.77143174e-03   3.08511173e-03   3.19681736e-03   4.27480042e-03\n",
      "   3.56297242e-03   2.96207075e-03   8.32712802e-04  -1.61455933e-03\n",
      "  -3.43778660e-03   1.26925937e-03   9.87906591e-04   2.98972498e-03\n",
      "  -6.95995041e-05  -8.87586037e-04  -1.01257616e-03  -1.61267794e-03\n",
      "   2.16175709e-03   1.21102983e-03   3.42706335e-03   4.66257706e-03\n",
      "   1.64661917e-03  -3.97273805e-03   1.56595698e-03  -3.57185747e-03\n",
      "   2.18323083e-03   4.15626727e-03  -7.79706577e-04  -2.53350567e-03\n",
      "   2.17486080e-03  -2.88217026e-03  -2.11121025e-03   2.48799194e-03\n",
      "   2.02176603e-03   4.88343881e-03   1.37916801e-03  -5.12963627e-04\n",
      "  -4.78631072e-03  -3.33486078e-03   1.24664849e-03  -2.58966908e-03\n",
      "   2.82074581e-03   9.47412569e-04   1.58476748e-03  -3.56739620e-03\n",
      "   7.21727265e-04   3.36474366e-03   3.99444671e-03   9.45772044e-04]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print model['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What do we have?   Word Embeddings \n",
    "\n",
    "**A word embedding W : words → ℝn **\n",
    "\n",
    "The output above is the result of 'word' projections in a latent spaces\n",
    "of N dimensions, (N ~ size of NN layers we chose).     \n",
    "Our float values above represent the coordinates for the word 'computer' in our 100-dimensional space!\n",
    "\n",
    "Our high dimensional vectors stand in place for words.    \n",
    "Note, that these dimensions are encoding 'latent' properties for 'computer' (such that 'queen' will be geometrically closer to 'king' than it would to be to (let's say) 'computer'. \n",
    "\n",
    "\n",
    "<img src='img/vector_queen.png'/>\n",
    "\n",
    "<img src='img/vector_queen2.png'/>\n",
    "\n",
    "\n",
    "Word Embeddings are useful because:\n",
    "\n",
    "1.  We can measure the semantic similarity between two words\n",
    "2.  We can use these word vectors as features for various NLP supervised learning tasks (such as classifcation, sentiment analysis). \n",
    "\n",
    "We will see how we get here.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### IDEA(S) # 3  Skip-Gram Methods &  CBOW Methods : \n",
    "\n",
    "#### Skip-Gram: \n",
    "\n",
    "**example sentence:**  \"We are on the cusp of deep learning for the masses\"\n",
    "\n",
    "a) Input of skip-gram is a single word (Wi) 'cusp' and the output are the words (Wo) in Wi's context window (defined by our word window C)\n",
    "\n",
    "Context window: \"We are on the cusp of deep\"  (Using Context Window = 5 & Skip-Gram=1)\n",
    "\n",
    "We will utilize 2 for loops: (1) iterating through our inputs (our 1st Wi will be : 'We')   \n",
    "(2) Our second for loop will iterate through our Wo's : {'are','the','cusp','of','deep'}\n",
    "\n",
    "b) One-hot encode both input & output vectors\n",
    "\n",
    "c) Our W matrix is just a massive matrix containing all of our word embeddings (row by row basis)\n",
    "\n",
    "d) In the process of learning weight matrices W & W', we initiate the matrices randomly.  \n",
    "\n",
    "e) We then sequenctially feed training examples into our model & observe the error (some function of the difference    between the expected output and the actual). \n",
    "\n",
    "f) We then compute the gradient of this error with respect to the elements of both matrices and correct them in the direction of the gradient (aha!  stochastic gradient descent).    As we learned with SGD, the goal is to take a small step (as controled by the learning rate) in order to minimize distance bewtween the vectors (Thereby, increase the probablity of P(wo|wi) \n",
    "\n",
    "g) We define our loss function.  (Our objective is to maximize the conditional probability of the output given our input context): \n",
    "\n",
    "h) By repeating this process over an entire training set, we will acquire vectors for words that habitually co_occur tend to be nudged closer together (and by gradually lowering the learning rate) this process converges towards some final state for the vectors.  \n",
    " \n",
    "<img src='img/skip_gram.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### CBOW: \n",
    "\n",
    "\n",
    "CBOW: very similiar model with the inputs & outputs reversed.  The input layer consists of our word window (Size C)\n",
    "\n",
    "<img src='img/CBOW.png'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Illustration.. \n",
    "\n",
    "import os\n",
    "\n",
    "class MySentences(object):\n",
    "     def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "         for fname in os.listdir(self.dirname):\n",
    "                for line in open(os.path.join(self.dirname, fname)):\n",
    "                    yield line.split()\n",
    "\n",
    "sentences = MySentences('/Users/julialintern/nltk_data/corpora/gutenberg') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences,min_count=3,workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prophet', 0.5949121713638306),\n",
       " ('beforetime', 0.5525686740875244),\n",
       " (\"king's\", 0.5368978977203369),\n",
       " ('impotent', 0.5293000936508179),\n",
       " ('prophet.', 0.5250883102416992)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can test our model\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "class MySentences2(object):\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "         for fname in os.listdir(self.dirname):\n",
    "                for line in open(os.path.join(self.dirname, fname)):\n",
    "                    word=line.lower().split()\n",
    "                    if word not in stop:\n",
    "                        yield word\n",
    "                    \n",
    "sentences = MySentences2('/Users/julialintern/nltk_data/corpora/gutenberg') \n",
    "model = gensim.models.Word2Vec(sentences,min_count=3,workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5984632968902588),\n",
       " ('paul', 0.5767810344696045),\n",
       " ('prince', 0.5267163515090942),\n",
       " ('corinthians', 0.5077587962150574),\n",
       " ('emperor', 0.5019620656967163)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can test our model\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57056338874927059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity\n",
    "\n",
    "model.similarity('woman','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68047994582732718"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine_similarity\n",
    "\n",
    "model.n_similarity(['woman', 'girl'], ['man', 'boy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19909103  0.11658057 -0.16124374 -0.0753553   0.280846   -0.33586395\n",
      " -0.09959811  0.07986737  0.09244389 -0.00216066  0.15571938 -0.15189013\n",
      "  0.01881346  0.40199128 -0.0604613   0.22984141  0.1188383  -0.00243269\n",
      "  0.02065047 -0.34811813 -0.08189397 -0.19340491  0.0159527  -0.01274251\n",
      " -0.09855368  0.07735041  0.05886059 -0.23424435  0.12708429  0.04924433\n",
      " -0.08262601 -0.16553693  0.15812439 -0.20336491 -0.16151191 -0.15788379\n",
      " -0.00684571  0.00556067 -0.24170582  0.35226527 -0.47907963 -0.10631083\n",
      " -0.3883279   0.07438131 -0.18755727 -0.17352945 -0.21855398  0.13600644\n",
      " -0.12352561 -0.22306238  0.03754399  0.15342307  0.45225978 -0.02662029\n",
      "  0.01002679 -0.11355209 -0.15838839 -0.33109254 -0.02738108 -0.17900626\n",
      " -0.13094853 -0.063792   -0.08352834 -0.03761846  0.09131543  0.04872246\n",
      "  0.11330067  0.03604474  0.16260488 -0.01536125  0.13360719  0.0912374\n",
      "  0.14135809 -0.00672222  0.17967939 -0.12232968 -0.18073294 -0.07064348\n",
      "  0.19077745  0.10241222  0.00484772  0.06955955  0.00646779  0.21972479\n",
      " -0.09060163  0.08373088  0.26626754 -0.03872798 -0.02934134  0.21225034\n",
      "  0.17747894 -0.25559098  0.00301975  0.01280602  0.23920521  0.11583488\n",
      "  0.10802481  0.35848314 -0.22554407 -0.00367589]\n"
     ]
    }
   ],
   "source": [
    "print model['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09021866 -0.01703615 -0.02865197  0.02772826 -0.00929441  0.03133548\n",
      "  0.05567687 -0.0292524   0.11020087  0.07419246  0.01350545 -0.07690042\n",
      "  0.11439198  0.00588876 -0.13341768 -0.02478784  0.00963654  0.00378606\n",
      " -0.03490182  0.00524183  0.08824384 -0.04764377 -0.0797544  -0.03424416\n",
      "  0.07177494 -0.02164323 -0.07376256 -0.11372524  0.15830228  0.07450563\n",
      " -0.04994248 -0.05376245  0.00284377 -0.08964711  0.04206511 -0.1294381\n",
      "  0.1155242   0.03627551  0.0479129   0.1475217   0.00926567 -0.06451008\n",
      "  0.03168656  0.04195825  0.00065164 -0.05297829 -0.07661257 -0.01360856\n",
      " -0.04855314 -0.10175702 -0.03983173  0.04128741  0.1897212   0.02466719\n",
      " -0.01337607  0.04385811 -0.13051809 -0.04868311 -0.04781685  0.05410472\n",
      "  0.00764993 -0.0131494  -0.10523718 -0.05013266  0.10316719  0.08746231\n",
      "  0.00569277  0.05293912 -0.06064452 -0.01950141  0.02286505  0.08702739\n",
      "  0.0897103   0.11196417  0.09544533  0.11684236  0.00404374 -0.08111674\n",
      " -0.05138755 -0.00552055  0.00084776  0.08524694 -0.04796741  0.10135489\n",
      " -0.05267575  0.11490574  0.09287292  0.0139473  -0.06216371  0.04112389\n",
      "  0.06873677 -0.04928437  0.00025953 -0.01212776  0.1246432  -0.0548622\n",
      "  0.13014421  0.06260595 -0.14657955  0.05301132]\n"
     ]
    }
   ],
   "source": [
    "print model['fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lunch'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like we should refine! \n",
    "\n",
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other very cool methods!: \n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But if you really want to refine your model, you'll need more data:\n",
    "\n",
    "\n",
    "https://code.google.com/p/word2vec/\n",
    "\n",
    "Download:  'freebase-vectors-skipgram1000-en.bin.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   How you can use Word2Vec in your models: \n",
    "\n",
    "1) Sum the word vectors in a sentence (like in CBOW) and predict the label. \n",
    "2) Sentiment analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Some things to keep in Mind when using Word2Vec:\n",
    "\n",
    "1) Word2vec requires a lot of data to train.\n",
    "\n",
    "As we've illustrated, you can download pretrained vectors. However, if you would need to train your own data \n",
    "you will need a lot of it!  (Think Hundreds of Millions of Words!) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTHER REFERENCES:\n",
    "\n",
    "- https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n",
    "- http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

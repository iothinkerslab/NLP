{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T07:49:19.131813Z",
     "start_time": "2018-06-01T07:49:14.054182Z"
    },
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varru\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "\n",
    "Today, we'll map text documents from a highly dimensional word (or token) space into a much reduced **semantic space** which allows us to make valuable **conceptual comparisons** between arbitrary blocks of text in this new vector space.\n",
    "\n",
    "Thus the **input is a large corpus of text documents** and the **output is a reduced semantic space for those input documents and words**.  \n",
    "On a [**document-term matrix**](https://en.wikipedia.org/wiki/Document-term_matrix) with [**TFIDF Weightings**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to map all the terms in the corpus into a reduced **term space** and all the documents into a reduced **document space**.  \n",
    "    - The 2 spaces are related by a simple transformation, so we can perform arbitrary **term-term**, **doc-doc**, and **doc-term comparisons** via [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "    - These 2 spaces make up the **\"dual space\"**\n",
    "        - Every document is the weighted sum of all of its terms\n",
    "        - Every term is the weighted sum of all the documents it occurs in (very useful!)\n",
    "\n",
    "[**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec) - uses a neural network to yield **term space**\n",
    "    - Has additional nice properties of term vectors, such as conceptual additivity (see below)\n",
    "\n",
    "## Goals &zwnj;\n",
    "\n",
    "- Use Word2vec to create a vector space for words in a training set\n",
    "- Use the Word2vec space to do simple comparisons between different combinations of words\n",
    "- Discuss various other considerations, tasks, and extensions for VSMs like LSI, NMF, and Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "- Vector Space Models\n",
    "  - What?\n",
    "  - Why?\n",
    "  - How?\n",
    "- TFIDF\n",
    "\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are Vector Space Models?\n",
    "- Map raw text into vector space\n",
    "- Allow semantic (conceptual) comparison between text chunks\n",
    "- **Input**: Corpus of raw text documents\n",
    "- **Output**: Vectors for text documents (and usually terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need Vector Space Models?\n",
    "- Early NLP (1950s-1980s): focused on **linguistics** and **handwritten rules**\n",
    "  - Extremely complex, impossible to maintain/scale\n",
    "- 1980s: ML introduced for NLP\n",
    "  - Linguistics $\\rightarrow$ \"***Corpus Linguistics***\"\n",
    "  - Learn from text via large ***corpora*** of labeled raw text documents\n",
    "- **Idea**: Map text to mathematical entities $\\rightarrow$ ***Word Vectors***\n",
    "- No more complex rules systems! $\\rightarrow$ *Unsupervised*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do VSMs work?\n",
    "- Start with raw text\n",
    "- Translate text to vectors (e.g. Counts, TFIDF)\n",
    "- Optional (Probable): **Reduce the space**\n",
    "  - Use Probabilistic Inference (LDA)\n",
    "  - Use Dimensionality Reduction via **Matrix Factorization** (SVD, NMF)\n",
    "  - Use a Neural Network (Word2Vec)\n",
    "- Once we have \"Semantic\" (meaning) vectors (**word vectors**):\n",
    "  - Do all sorts of ML with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of VSMs\n",
    "- **LDA**:\n",
    "  - Word Space $\\rightarrow$ Topic Space via Bayesian Inference\n",
    "- **TFIDF**:\n",
    "  - Word Space not reduced, but augmented\n",
    "- **LSA/LSI** and **NMF**:\n",
    "  - Word Space $\\rightarrow$ Semantic Space via SVD or NMF \n",
    "- **Word2Vec**:\n",
    "  - Word Space $\\rightarrow$ Semantic Space via Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency Inverse Document Frequency (TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TFIDF\n",
    "- Creates vectors for documents based on unique term counts (frequencies)\n",
    "- Weights the frequency counts by TFIDF weighting\n",
    "- Does **not** reduce dimensionality\n",
    "- Frequent preprocessing step for matrix factorization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-Document Matrix (TDM)\n",
    "- Start with corpus of documents (raw text)\n",
    "- Create matrix:\n",
    "  - Rows are our **term vocabulary** - unique terms over all documents\n",
    "  - Columns are all documents\n",
    "  - Entries are respective term frequency for each document\n",
    "- \"Terms\" means tokens, whatever is extracted from tokenization in preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TFIDF Weighting\n",
    "- Term Frequency Inverse Document Frequency\n",
    "- Common weighting scheme applied to term-document matrix\n",
    "- Any function that is:\n",
    "  - Directly proportional to term frequency **within document** (local weight)\n",
    "  - Inversely proportional to term frequency **in all documents** (global weight)\n",
    "- **Motivation**: Highly common terms are not useful to distinguish documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TFIDF: Relation to VSMs\n",
    "- TFIDF vectors are a VSM on their own! \n",
    "- TFIDF weightings empirically improve VSMs\n",
    "- Some VSMs (LSI, NMF) almost certainly require this step, some don't (LDA, Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing Considerations\n",
    "- Tokenization: Determines our unique terms $\\rightarrow$ size of matrix\n",
    "- Stopwords\n",
    "- Stemming\n",
    "- Named Entity Recognition\n",
    "- Punctuation\n",
    "- Min/Max Frequency Threshold (how often should term occur to keep it?)\n",
    "- Phrase Extraction\n",
    "- Part-of-Speech Tagging\n",
    "- Word-sense Disambiguation (bush vs George Bush)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common TFIDF Weighting Scheme\n",
    "- **Entropy**:\n",
    "  - **Term Frequency** for term $t$ in a specific document: $tf_{t}$ i.e. $\\frac{count(term_t)}{len(doc)}$\n",
    "  - **Document Frequency** for term $t$: fraction of documents that contain term $t$: $df_t$ i.e. $\\frac{n_t}{N}$, where $n_t$ is the number of documents that term t occurs in and N is the total number of documents in the corpus.\n",
    "  - TFIDF = term-frequency **inverse** document frequency, usually with smoothing and log scaling\n",
    "  - TFIDF Weight: \n",
    "$$\n",
    "tfidf = (1 + \\log{tf_t}) \\log(\\frac{1}{df_t}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\qquad \\qquad = (1 + \\log{tf_t}) \\log(\\frac{N}{n_t})\n",
    "$$\n",
    "\n",
    "The intuition behind tf-idf weighting is that frequent use of a word is only important relative to how common that word is across all documents. For example, if I have a document with the word \"the\" occuring a bunch of times, we shouldn't conclude much from that because \"the\" occurs in pretty much all the documents (**high tf but also high df**). But if I have a document with the word \"correlation\" occuring 50% of the time when it only occurs in 3% of documents (**high tf and low df**), I can now treat that as much more important information.\n",
    "\n",
    "Tf-idf has an information theoretic/probabilistic interpretation as calculating the cross-entropy between the global distribution of a term across the corpus and the local distribution of the term in a document. [See here for more](http://searchivarius.org/blog/tf-idf-simply-cross-entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**TFIDF in `sklearn`**: \n",
    "Let's implement TFIDF with the [20 Newsgroups Dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "- Here's TFIDF in `sklearn` with `TfidfVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T07:49:23.649955Z",
     "start_time": "2018-06-01T07:49:20.538638Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aau</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zq</th>\n",
       "      <th>zr</th>\n",
       "      <th>zs</th>\n",
       "      <th>zubov</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zx</th>\n",
       "      <th>zyeh</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13776 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aardvark  aaron  aau   ab  abandon  abandoned  abbreviation  abc  \\\n",
       "0  0.0  0.0       0.0    0.0  0.0  0.0      0.0        0.0           0.0  0.0   \n",
       "1  0.0  0.0       0.0    0.0  0.0  0.0      0.0        0.0           0.0  0.0   \n",
       "2  0.0  0.0       0.0    0.0  0.0  0.0      0.0        0.0           0.0  0.0   \n",
       "3  0.0  0.0       0.0    0.0  0.0  0.0      0.0        0.0           0.0  0.0   \n",
       "4  0.0  0.0       0.0    0.0  0.0  0.0      0.0        0.0           0.0  0.0   \n",
       "\n",
       "  ...   zoom   zq   zr   zs  zubov  zuma  zurich   zx  zyeh   zz  \n",
       "0 ...    0.0  0.0  0.0  0.0    0.0   0.0     0.0  0.0   0.0  0.0  \n",
       "1 ...    0.0  0.0  0.0  0.0    0.0   0.0     0.0  0.0   0.0  0.0  \n",
       "2 ...    0.0  0.0  0.0  0.0    0.0   0.0     0.0  0.0   0.0  0.0  \n",
       "3 ...    0.0  0.0  0.0  0.0    0.0   0.0     0.0  0.0   0.0  0.0  \n",
       "4 ...    0.0  0.0  0.0  0.0    0.0   0.0     0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 13776 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the 20 Newsgroups data\n",
    "ng = datasets.fetch_20newsgroups()\n",
    "# Get the raw text docs\n",
    "ng_text = ng.data\n",
    "\n",
    "# Vectorize the text using TFIDF\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", \n",
    "                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", #words with >= 2 alpha chars \n",
    "                        min_df=10)\n",
    "tfidf_vecs = tfidf.fit_transform(ng_text)\n",
    "pd.DataFrame(tfidf_vecs.todense(), \n",
    "             columns=tfidf.get_feature_names()\n",
    "            ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications of TFIDF\n",
    "- We have a vector space!\n",
    "  - Document vectors are the rows\n",
    "  - Term vectors are the columns\n",
    "- We can try to do ML\n",
    "  - e.g.: Naive Bayes for Text Classification\n",
    "- **BUT**...\n",
    "  - Many unique terms $\\rightarrow$ many unique features!\n",
    "  - Curse of Dimensionality :(\n",
    "  - Dimensionality Reduction seems prudent :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Text Classification\n",
    "- Naive Bayes empirically avoids the curse somewhat on TFIDF vectors\n",
    "  - Performs decent enough on high-dimensional text classification tasks\n",
    "- How does it work?\n",
    "  - The observations are the weighted TFIDF vectors\n",
    "    - This is a (weighted) bag of words for document $j$ and terms $\\{w_i\\}$\n",
    "$$\n",
    "P(Class C | \\{w_i\\}) = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}} = \\frac{P(\\{w_i\\} | C) \\times P(C)}{P(\\{w_i\\})}\n",
    "$$\n",
    "- Naive Assumption: \n",
    "$$\n",
    "P(\\{w_i\\} | C) = \\prod\\limits_i P(w_i|C)\n",
    "$$\n",
    "- This is just a multinomial distribution where **given a class C each word has probability $p_i$ of appearing**!\n",
    "- Thus:\n",
    "  - **Likelihood is multinomial**:\n",
    "    - $P(w_i)|C)$: Number of times word $w_i$ appears in class C documents divided by total number of words in Class C documents\n",
    "  - **Prior**: Proportion of documents of class C\n",
    "  - We can use Multinomial Naive Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try simple Naive Bayes classification on TFIDF vectors from above in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T07:49:44.652766Z",
     "start_time": "2018-06-01T07:49:44.590258Z"
    },
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86582753079807173"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, \n",
    "                                                    ng.target, \n",
    "                                                    test_size=0.33)\n",
    "\n",
    "# Train \n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Test \n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is word2vec?\n",
    "- VSM for text analysis\n",
    "- **Input**: Corpus of text documents\n",
    "- **Output**: Reduced vector space for all terms(individual words) in documents, captures **Semantics** and **Syntax**.\n",
    "- **Training**: Neural Network based on sliding windows of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why word2vec?\n",
    "- Other VSMs (LSI, NMF, etc) **don't capture word order** (Bag of Words)!\n",
    "- It would be nice if we could, at least a little!\n",
    "- word2vec does somewhat\n",
    "- word2vec can capture analogy relationships:\n",
    "  - e.g.: King is to man as Queen is to woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does word2vec work?\n",
    "- We use a **shallow** (1 hidden layer) neural network \n",
    "- Train on \"**context windows**\", small sequences of words in text (5-10 maybe)\n",
    "- The input is either:\n",
    "  - A word in the context window (\"Skip-Grams\") - **predicting the context given a word**\n",
    "  - All words but 1 in a context window (\"CBOW\") - **predicting a word given the context**\n",
    "- The output is either:\n",
    "  - All the other words in a context window (\"Skip-Grams\")\n",
    "  - The missing word from the context window (\"CBOW\")\n",
    "  \n",
    "So if we take a sentence: \"Varun hopped into the TeslaCar\" we would be taking one of these classification approaches:\n",
    "\n",
    "* **Skip-Grams**: using into, predict Varun, hopped, the, and TeslaCar\n",
    "* **CBOW**: using Varun, hopped, the, and Tesla, predict into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing Considerations\n",
    "- Everything from our previous VSMs!\n",
    "- aka Tokenization, stemming, stopwords, Entity extraction, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2Vec\n",
    "- Use a neural network on context windows\n",
    "- 2 main approaches for inputs and labels:\n",
    "  - **Skip-Grams**\n",
    "  - **Continuous Bag of Words (CBOW)**\n",
    "  - Vectors usually similar, subtle differences, also differences in computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Context Windows\n",
    "- **Observations** for word2vec: All **context windows** in a corpus\n",
    "  - Size of a context window is a chosen parameter\n",
    "- e.g.:\n",
    "  - Document: \"The quick brown fox jumped over the lazy dog.\"\n",
    "  - Window size: 5\n",
    "  - Window 1: \"**The quick brown fox jumped** over the lazy dog.\"\n",
    "  - Window 2: \"The **quick brown fox jumped over** the lazy dog.\"\n",
    "  - Window 3: \"The quick **brown fox jumped over the** lazy dog.\"\n",
    "  - Window 4: \"The quick brown **fox jumped over the lazy** dog.\"\n",
    "  - Window 5: \"The quick brown fox **jumped over the lazy dog**.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### \"One-hot\" Encoding Word Vectors\n",
    "- We need to be able to represent a **sequence of words** as a vector\n",
    "- To do this, we need to assign each word an index from 0 to V\n",
    "  - V is the size of the vocabulary aka # distinct words in the corpus\n",
    "- A **word vector** is:\n",
    "  - 1 for the index of that word\n",
    "  - 0 for all other entries  \n",
    "<img src='1-hot.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-hot Encoding Context Windows\n",
    "- We need vectors for context windows\n",
    "- A sequence of words will have a vector that's just the concatenation of its word vectors\n",
    "  - Thus, for window size $d$ the vector is of length $V \\times d$\n",
    "  - Only $d$ entries (one for each word) will be nonzero (1s)\n",
    "  \n",
    "<img src='catdog.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skip-Grams\n",
    "- Build a neural network with 1 hidden layer\n",
    "- **Inputs**: \n",
    "  - The middle word of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V$\n",
    "- **Outputs**: \n",
    "  - The other words of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V \\times (d-1)$\n",
    "- Turn the crank!  \n",
    "<img src='skip_gram.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous Bag of Words (CBOW)\n",
    "- Build a neural network with 1 hidden layer\n",
    "- Just reverse of Skip-Grams!\n",
    "- **Inputs**: \n",
    "  - The other words of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V \\times (d-1)$\n",
    "- **Outputs**: \n",
    "  - The middle word of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V$\n",
    "- Turn the crank!  \n",
    "<img src='cbow.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dimensionality Reduction\n",
    "- Number of nodes in hidden layer, $N$, is a parameter\n",
    "- It is the (reduced) dimensionality of our resulting word vector space!\n",
    "- Fit the neural net $\\rightarrow$ find weights matrix $W$\n",
    "  - In the new space, $x_N = W^Tx$\n",
    "  - Checking dimensions:\n",
    "    - $x$: $V \\times 1$\n",
    "    - $W^T$: $N \\times V$\n",
    "    - $x_N$: $N \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### So What is Happening?\n",
    "- We're learning the words likely to appear near each word\n",
    "- This context information ultimately leads to vectors for related words falling near one another!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nice Properties of Word2Vec Vectors\n",
    "- word2vec (somewhat magically!) captures nice geometric relations between words\n",
    "<img src='vector_queen2.png' align='right'/>\n",
    "- e.g.: Analogies\n",
    "  - King is to Queen as man is to woman\n",
    "  - The vector between King and Queen is the same as that between man and woman!\n",
    "- Works for all sorts of things: capitals, cities, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec for ML\n",
    "- Again we get **word vectors**!\n",
    "- So we can use them for ML!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using Existing Word Vectors\n",
    "- word2vec takes **A LOT** of data to train it well\n",
    "- What if you don't have **A LOT** of data?\n",
    "  - Steal someone else's vectors!\n",
    "  - Google has trained a [giant set](https://code.google.com/p/word2vec/)\n",
    "  - So has [Stanford NLP](http://nlp.stanford.edu/projects/glove/) (slightly different, same idea)\n",
    "  - Many others\n",
    "- As long as the domain is similar, should be better than yours\n",
    "  - Need words to have the same meaning as in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### word2vec in `gensim`\n",
    "- Very simple example for usage\n",
    "- We'll train our own (don't!  steal vectors!)\n",
    "- It needs a list of lists representing the sentences in a corpus\n",
    "- Here's how that goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T16:56:07.375468Z",
     "start_time": "2018-06-01T16:56:07.344214Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Make sure gensim and Word2Vec are installed and functional\n",
    "# Create some dummy data\n",
    "sentences = documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "\n",
    "# The type of input that Word2Vec is looking for.. \n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T16:56:07.766135Z",
     "start_time": "2018-06-01T16:56:07.672374Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-01 22:26:07,672 : INFO : collecting all words and their counts\n",
      "2018-06-01 22:26:07,672 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-01 22:26:07,672 : INFO : collected 35 word types from a corpus of 52 raw words and 9 sentences\n",
      "2018-06-01 22:26:07,672 : INFO : Loading a fresh vocabulary\n",
      "2018-06-01 22:26:07,672 : INFO : min_count=1 retains 35 unique words (100% of original 35, drops 0)\n",
      "2018-06-01 22:26:07,672 : INFO : min_count=1 leaves 52 word corpus (100% of original 52, drops 0)\n",
      "2018-06-01 22:26:07,672 : INFO : deleting the raw counts dictionary of 35 items\n",
      "2018-06-01 22:26:07,672 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2018-06-01 22:26:07,688 : INFO : downsampling leaves estimated 11 word corpus (21.7% of prior 52)\n",
      "2018-06-01 22:26:07,688 : INFO : estimated required memory for 35 words and 100 dimensions: 45500 bytes\n",
      "2018-06-01 22:26:07,688 : INFO : resetting layer weights\n",
      "2018-06-01 22:26:07,688 : INFO : training model with 4 workers on 35 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-01 22:26:07,688 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:07,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:07,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:07,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:07,688 : INFO : EPOCH - 1 : training on 52 raw words (13 effective words) took 0.0s, 2119 effective words/s\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:07,703 : INFO : EPOCH - 2 : training on 52 raw words (15 effective words) took 0.0s, 2506 effective words/s\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:07,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:07,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:07,719 : INFO : EPOCH - 3 : training on 52 raw words (15 effective words) took 0.0s, 2292 effective words/s\n",
      "2018-06-01 22:26:07,719 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:07,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:07,719 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:07,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:07,719 : INFO : EPOCH - 4 : training on 52 raw words (8 effective words) took 0.0s, 1328 effective words/s\n",
      "2018-06-01 22:26:07,719 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:07,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:07,734 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:07,734 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:07,734 : INFO : EPOCH - 5 : training on 52 raw words (8 effective words) took 0.0s, 1243 effective words/s\n",
      "2018-06-01 22:26:07,734 : INFO : training on a 260 raw words (59 effective words) took 0.0s, 1256 effective words/s\n",
      "2018-06-01 22:26:07,734 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\varru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.65059741e-04,  -3.45909828e-03,  -7.98419351e-05,\n",
       "         1.47965609e-03,  -2.72638397e-03,   5.03118266e-04,\n",
       "        -2.71530892e-03,  -1.54730002e-03,   1.33191678e-03,\n",
       "         3.11394455e-03,   2.37710075e-03,  -1.93741685e-03,\n",
       "        -2.04316876e-03,   3.37618752e-03,  -5.85331918e-07,\n",
       "        -3.17614898e-03,   2.09084200e-03,  -9.49236564e-04,\n",
       "        -3.75025650e-03,  -3.27538364e-05,  -3.04771168e-03,\n",
       "         4.74398863e-03,  -3.69384652e-03,  -1.26163941e-03,\n",
       "        -4.41896170e-03,  -3.53475707e-03,  -3.99893522e-03,\n",
       "         3.46871163e-03,  -1.51816325e-03,   3.19902855e-03,\n",
       "        -2.94388062e-03,  -4.29868745e-03,  -3.87685490e-03,\n",
       "         2.84234085e-03,   4.10572626e-03,  -4.53026406e-03,\n",
       "         4.05600807e-03,   3.75391566e-03,   6.45549502e-04,\n",
       "        -2.98501947e-03,   4.64301789e-03,   1.36235240e-03,\n",
       "         3.65666370e-03,   2.85392255e-03,  -2.83774198e-03,\n",
       "        -1.41713172e-04,   4.12686961e-03,  -2.78024725e-03,\n",
       "        -1.20313047e-03,   1.69402722e-03,   2.05388642e-04,\n",
       "        -3.54007585e-03,   2.38605868e-03,  -4.27056290e-03,\n",
       "         3.63526610e-03,  -3.91926290e-03,  -2.44738488e-03,\n",
       "         5.12906641e-04,   4.11616778e-03,   4.71585401e-04,\n",
       "        -4.22810204e-03,   3.07891797e-03,  -1.25623937e-03,\n",
       "        -3.27687548e-03,  -3.60389426e-03,   5.69827738e-04,\n",
       "        -3.77292046e-03,   2.71226218e-05,  -2.19044555e-03,\n",
       "         9.27016779e-04,   3.57071194e-03,  -2.85931310e-04,\n",
       "         1.15790535e-04,   1.83403504e-03,   1.01052457e-03,\n",
       "        -4.08630027e-03,  -3.63388960e-03,  -3.44736292e-03,\n",
       "         1.06361858e-03,  -2.44448683e-03,  -3.73329897e-03,\n",
       "        -2.25875364e-03,   3.71467066e-03,   1.41936110e-03,\n",
       "         4.95035434e-03,   4.57433239e-03,   4.79743909e-03,\n",
       "        -4.66425996e-03,  -3.00127757e-03,  -5.06797805e-04,\n",
       "         3.12852790e-03,  -3.29227163e-03,   6.42279920e-05,\n",
       "        -3.36702145e-03,   4.71472600e-03,   3.51895578e-03,\n",
       "        -1.53054204e-03,   7.90938560e-04,  -1.99902267e-03,\n",
       "        -2.41719023e-03], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train word2vec\n",
    "w2v = models.Word2Vec(texts, size=100, window=5, min_count=1, workers=4,sg=1)\n",
    "# Check out the resulting vector for \"computer\"\n",
    "w2v['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now on a slightly Larger Corpus**:  \n",
    "- Using Project Gutenberg Corpus (Books) from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T16:56:21.363755Z",
     "start_time": "2018-06-01T16:56:08.421417Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-01 22:26:08,421 : INFO : collecting all words and their counts\n",
      "2018-06-01 22:26:08,421 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-01 22:26:08,468 : INFO : PROGRESS: at sentence #10000, processed 93960 words, keeping 12185 word types\n",
      "2018-06-01 22:26:08,515 : INFO : PROGRESS: at sentence #20000, processed 189593 words, keeping 19551 word types\n",
      "2018-06-01 22:26:08,546 : INFO : PROGRESS: at sentence #30000, processed 278383 words, keeping 24115 word types\n",
      "2018-06-01 22:26:08,577 : INFO : PROGRESS: at sentence #40000, processed 359433 words, keeping 27838 word types\n",
      "2018-06-01 22:26:08,608 : INFO : PROGRESS: at sentence #50000, processed 443441 words, keeping 33634 word types\n",
      "2018-06-01 22:26:08,655 : INFO : PROGRESS: at sentence #60000, processed 532123 words, keeping 36348 word types\n",
      "2018-06-01 22:26:08,687 : INFO : PROGRESS: at sentence #70000, processed 618856 words, keeping 39209 word types\n",
      "2018-06-01 22:26:08,718 : INFO : PROGRESS: at sentence #80000, processed 704611 words, keeping 41803 word types\n",
      "2018-06-01 22:26:08,765 : INFO : PROGRESS: at sentence #90000, processed 777856 words, keeping 44732 word types\n",
      "2018-06-01 22:26:08,780 : INFO : PROGRESS: at sentence #100000, processed 847244 words, keeping 47843 word types\n",
      "2018-06-01 22:26:08,827 : INFO : PROGRESS: at sentence #110000, processed 935550 words, keeping 49530 word types\n",
      "2018-06-01 22:26:08,874 : INFO : PROGRESS: at sentence #120000, processed 1019480 words, keeping 51150 word types\n",
      "2018-06-01 22:26:08,921 : INFO : PROGRESS: at sentence #130000, processed 1100045 words, keeping 52754 word types\n",
      "2018-06-01 22:26:08,952 : INFO : PROGRESS: at sentence #140000, processed 1181718 words, keeping 54727 word types\n",
      "2018-06-01 22:26:08,983 : INFO : PROGRESS: at sentence #150000, processed 1262808 words, keeping 59962 word types\n",
      "2018-06-01 22:26:09,019 : INFO : PROGRESS: at sentence #160000, processed 1346801 words, keeping 66106 word types\n",
      "2018-06-01 22:26:09,044 : INFO : PROGRESS: at sentence #170000, processed 1437473 words, keeping 72473 word types\n",
      "2018-06-01 22:26:09,091 : INFO : PROGRESS: at sentence #180000, processed 1527593 words, keeping 76665 word types\n",
      "2018-06-01 22:26:09,138 : INFO : PROGRESS: at sentence #190000, processed 1614568 words, keeping 81222 word types\n",
      "2018-06-01 22:26:09,169 : INFO : PROGRESS: at sentence #200000, processed 1707504 words, keeping 87681 word types\n",
      "2018-06-01 22:26:09,216 : INFO : PROGRESS: at sentence #210000, processed 1801526 words, keeping 94824 word types\n",
      "2018-06-01 22:26:09,263 : INFO : PROGRESS: at sentence #220000, processed 1887332 words, keeping 101856 word types\n",
      "2018-06-01 22:26:09,294 : INFO : PROGRESS: at sentence #230000, processed 1959345 words, keeping 108081 word types\n",
      "2018-06-01 22:26:09,326 : INFO : PROGRESS: at sentence #240000, processed 2016424 words, keeping 114684 word types\n",
      "2018-06-01 22:26:09,357 : INFO : PROGRESS: at sentence #250000, processed 2089258 words, keeping 120629 word types\n",
      "2018-06-01 22:26:09,388 : INFO : collected 124601 word types from a corpus of 2136725 raw words and 257115 sentences\n",
      "2018-06-01 22:26:09,388 : INFO : Loading a fresh vocabulary\n",
      "2018-06-01 22:26:09,607 : INFO : min_count=3 retains 38831 unique words (31% of original 124601, drops 85770)\n",
      "2018-06-01 22:26:09,607 : INFO : min_count=3 leaves 2034270 word corpus (95% of original 2136725, drops 102455)\n",
      "2018-06-01 22:26:09,716 : INFO : deleting the raw counts dictionary of 124601 items\n",
      "2018-06-01 22:26:09,716 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2018-06-01 22:26:09,732 : INFO : downsampling leaves estimated 1545941 word corpus (76.0% of prior 2034270)\n",
      "2018-06-01 22:26:09,857 : INFO : estimated required memory for 38831 words and 100 dimensions: 50480300 bytes\n",
      "2018-06-01 22:26:09,857 : INFO : resetting layer weights\n",
      "2018-06-01 22:26:10,435 : INFO : training model with 5 workers on 38831 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-01 22:26:11,435 : INFO : EPOCH 1 - PROGRESS: at 44.79% examples, 707506 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:12,451 : INFO : EPOCH 1 - PROGRESS: at 87.72% examples, 697496 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:12,639 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-01 22:26:12,639 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:12,639 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:12,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:12,654 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:12,654 : INFO : EPOCH - 1 : training on 2136725 raw words (1546022 effective words) took 2.2s, 697072 effective words/s\n",
      "2018-06-01 22:26:13,669 : INFO : EPOCH 2 - PROGRESS: at 44.33% examples, 697582 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:14,672 : INFO : EPOCH 2 - PROGRESS: at 88.27% examples, 699684 words/s, in_qsize 0, out_qsize 1\n",
      "2018-06-01 22:26:14,860 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-01 22:26:14,860 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:14,860 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:14,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:14,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:14,876 : INFO : EPOCH - 2 : training on 2136725 raw words (1546018 effective words) took 2.2s, 697696 effective words/s\n",
      "2018-06-01 22:26:15,876 : INFO : EPOCH 3 - PROGRESS: at 46.18% examples, 730399 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:16,891 : INFO : EPOCH 3 - PROGRESS: at 97.20% examples, 754544 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:16,923 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-01 22:26:16,923 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:16,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:16,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:16,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:16,942 : INFO : EPOCH - 3 : training on 2136725 raw words (1545931 effective words) took 2.1s, 752178 effective words/s\n",
      "2018-06-01 22:26:17,942 : INFO : EPOCH 4 - PROGRESS: at 46.18% examples, 725368 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:18,957 : INFO : EPOCH 4 - PROGRESS: at 90.13% examples, 709114 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:19,129 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-01 22:26:19,129 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:19,129 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:19,129 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:19,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:19,129 : INFO : EPOCH - 4 : training on 2136725 raw words (1545786 effective words) took 2.2s, 703239 effective words/s\n",
      "2018-06-01 22:26:20,144 : INFO : EPOCH 5 - PROGRESS: at 45.70% examples, 723275 words/s, in_qsize 1, out_qsize 0\n",
      "2018-06-01 22:26:21,144 : INFO : EPOCH 5 - PROGRESS: at 87.72% examples, 696859 words/s, in_qsize 0, out_qsize 0\n",
      "2018-06-01 22:26:21,332 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-01 22:26:21,332 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-01 22:26:21,332 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-01 22:26:21,348 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-01 22:26:21,348 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-01 22:26:21,348 : INFO : EPOCH - 5 : training on 2136725 raw words (1546111 effective words) took 2.2s, 699227 effective words/s\n",
      "2018-06-01 22:26:21,348 : INFO : training on a 10683625 raw words (7729868 effective words) took 10.9s, 708209 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# An Illustration.. \n",
    "\n",
    "import os\n",
    "\n",
    "# Create an iterator Class that can be used as a gensim corpus (defines how to read in the text data)\n",
    "class MySentences(object):\n",
    "     def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "         for fname in os.listdir(self.dirname):\n",
    "                for line in open(\n",
    "                    os.path.join(self.dirname, fname), \n",
    "                    encoding='utf-8', errors='ignore'):\n",
    "                    yield line.split()\n",
    "\n",
    "# Instantiate the corpus from a text file of documents\n",
    "# You'll need to change the path!\n",
    "sentences = MySentences('/Users/varru/nltk_data/corpora/gutenberg') # a memory-friendly iterator\n",
    "# Create a Word2vec model\n",
    "w2v = models.Word2Vec(sentences,min_count=3,workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Similarities\n",
    "- Have word vectors for all terms!  \n",
    "- Gensim provides a number of methods to then do comparisons in this vector space \n",
    "- Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T16:56:21.441888Z",
     "start_time": "2018-06-01T16:56:21.363755Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "2018-06-01 22:26:21,379 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Moses', 0.760481059551239),\n",
       " ('Solomon', 0.7598444223403931),\n",
       " ('David', 0.7492145299911499),\n",
       " ('captain', 0.7483667135238647),\n",
       " ('son', 0.7464020848274231),\n",
       " ('Joshua', 0.723465621471405),\n",
       " ('elders', 0.7191511988639832),\n",
       " ('Jesus', 0.7033897638320923),\n",
       " ('daughter', 0.7005856037139893),\n",
       " ('messengers', 0.6951842308044434)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words close to woman and king but not man\n",
    "w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Not so hot!**  \n",
    "- Unsurprising: Need much more data\n",
    "- A lesson in why you should steal vectors :)\n",
    "- Let's try some pairwise comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T07:59:25.237429Z",
     "start_time": "2018-06-01T07:59:25.221931Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755077698653\n",
      "0.796434537654\n",
      "man\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\Users\\varru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\varru\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Similarity between woman and man\n",
    "print(w2v.similarity('woman','man'))\n",
    "\n",
    "# Similarity between bags of words\n",
    "print(w2v.n_similarity(['woman', 'girl'], ['man', 'boy']))\n",
    "\n",
    "# Finding words that don't match others in a bag\n",
    "print(w2v.doesnt_match(\"breakfast man dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec for Text Clustering\n",
    "- We won't do it here, but the procedure is similar to LSI above. The key difference is that now we **only have vectors for each word** instead of vectors for entire documents. We could get back to the document level by averaging across all of the word vectors in each document that we extract from our gensim model.\n",
    "\n",
    " - Try whatever clustering algorithm you like on the reduced document vectors\n",
    "- **OR** more likely you just take the vectors as given from Google or Stanford and do the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec for Text Classification\n",
    "- We won't do it here, but the procedure is similar to LSI above. The key difference is that now we **only have vectors for each word** instead of vectors for entire documents. We could get back to the document level by averaging across all of the word vectors in each document that we extract from our gensim model.\n",
    "\n",
    " - Try whatever classifying algorithm you like on the reduced vectors\n",
    "    - For example, KNN with Cosine Similarity\n",
    "- **OR** more likely you just take the vectors as given from Google or Stanford and do the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Word Vectors (VSMs) \n",
    "- With word vectors, we can do so many cool things:\n",
    "  - ML algorithms\n",
    "  - Machine Translation\n",
    "  - Many of those things mentioned on NLP day 1\n",
    "  - Seed Deep Learning with them to do **even cooler stuff**\n",
    "- Basically, we know the state of the world (the meaning of words)...\n",
    "  - The possibilities are endless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "metis",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
